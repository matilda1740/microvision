{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6a98f6",
   "metadata": {},
   "source": [
    "## MicroVision\n",
    "\n",
    "### Log Parsing Module using Drain3 \n",
    "### Log Enrichment - Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b0408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed!!\n",
      "Current working directory: /Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd() == '/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision/notebooks':\n",
    "    os.chdir('/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision')\n",
    "    print(\"Changed!!\")\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306533c9",
   "metadata": {},
   "source": [
    "#### Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc4c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install drain3 pandas matplotlib tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9cba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Drain3 and dependencies imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.file_persistence import FilePersistence\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.template_miner_config import MaskingInstruction\n",
    "\n",
    "print(\"‚úÖ Drain3 and dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e97fe7",
   "metadata": {},
   "source": [
    "#### Configurations and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3740314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: data/OpenStack_2k.log\n",
      "The extracted templates (TEMPLATE-LEVEL) will be written to: data/OpenStack_2k.log_templates.csv\n",
      "The structured log templates enriched with metadata will be written to: data/OpenStack_2k.log_structured.csv\n",
      "Persistence path (Drain3) data/persistence/drain3_state\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"DATA_DIR\": \"data\",\n",
    "        \"DATASET_NAME\": \"OpenStack\",\n",
    "        # \"DATASET_LOG\": \"_full.log\",\n",
    "        \"DATASET_LOG\": \"_2k.log\",\n",
    "\n",
    "        \"OUTPUT_CSV\": \"_structured.csv\",\n",
    "        \"TEMPLATES_CSV\": \"_templates.csv\",\n",
    "        \"CLEANED_CSV\": \"_cleaned_templates.csv\",\n",
    "        \"MAX_LINES\": None,  # Set to None to process all lines\n",
    "        # \"MAX_LINES\": 2000,\n",
    "        \"PERSISTENCE_PATH\": \"persistence\",\n",
    "        \"DRAIN_PATH\": \"drain3_state\",\n",
    "        # \"ENRICHED_CSV\": \"_enriched.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "DATASET_PATH = f\"{config['DATA_DIR']}/{config['DATASET_NAME']+config['DATASET_LOG']}\"\n",
    "\n",
    "# The output file will contain the structured log level csv\n",
    "OUTPUT_CSV = f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['OUTPUT_CSV']}\"\n",
    "TEMPLATES_CSV = f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['TEMPLATES_CSV']}\"\n",
    "CLEANED_CSV = f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['CLEANED_CSV']}\"\n",
    "\n",
    "\n",
    "persistence_dir = os.path.join(f\"{config['DATA_DIR']}/{config['PERSISTENCE_PATH']}\", config['DRAIN_PATH'])\n",
    "os.makedirs(persistence_dir, exist_ok=True)\n",
    "persistence = FilePersistence(f\"{persistence_dir}/drain3_state.bin\")\n",
    "\n",
    "print(\"Dataset Path:\", DATASET_PATH)\n",
    "print(\"The extracted templates (TEMPLATE-LEVEL) will be written to:\", TEMPLATES_CSV)\n",
    "print(\"The structured log templates enriched with metadata will be written to:\", OUTPUT_CSV)\n",
    "print(\"Persistence path (Drain3)\", f\"{persistence_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bedae",
   "metadata": {},
   "source": [
    "### Utility Functions for Log Parsing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd2b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "# ------- UF: Loading Raw Log files\n",
    "\n",
    "def load_raw_logs(log_dir: str, dataset_name: str):\n",
    "    log_files = glob.glob(f\"{log_dir}/{dataset_name}*.log\")\n",
    "    raw_logs = []\n",
    "    for file_path in log_files:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            raw_logs.extend(f.readlines())\n",
    "    return raw_logs\n",
    "\n",
    "\n",
    "# ------- UF: Convert LogPai-style format to regex\n",
    "\n",
    "def log_format_to_regex(log_format: str) -> str:\n",
    "    tokens = re.findall(r\"<([^>]+)>\", log_format)\n",
    "    regex = re.escape(log_format)\n",
    "    for t in tokens:\n",
    "        esc = re.escape(f\"<{t}>\")\n",
    "        if t.lower() == \"content\":\n",
    "            repl = rf\"(?P<{t}>.*)\"\n",
    "        else:\n",
    "            repl = rf\"(?P<{t}>.+?)\"\n",
    "        regex = regex.replace(esc, repl, 1)\n",
    "    regex = regex.replace(r\"\\ \", r\"\\s+\")\n",
    "    return rf\"^{regex}$\"\n",
    "\n",
    "\n",
    "# ------- UF: Parse log line with given format \n",
    "\n",
    "def parse_line_with_format(line: str, log_format: str):\n",
    "    \"\"\"Return dict of matched groups (or {'Content': line} fallback).\"\"\"\n",
    "    if not log_format:\n",
    "        return {\"Content\": line}\n",
    "    regex = log_format_to_regex(log_format)\n",
    "    line = line.strip()\n",
    "    m = re.match(regex, line)\n",
    "    if not m:\n",
    "        return {\"ParseError\": True, \"Raw\": line, \"Content\": line}\n",
    "    return m.groupdict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a9d8e",
   "metadata": {},
   "source": [
    "### Dynamically Select Log Format Mapping for current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a529a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log format for OpenStack: <Date> <Time> <Pid> <Level> <Component> <Content>\n"
     ]
    }
   ],
   "source": [
    "# LOG FORMAT MAPPINGS FROM LogPai's GitHub\n",
    "log_format_mappings = {\n",
    "    \"OpenStack\": \"<Date> <Time> <Pid> <Level> <Component> <Content>\",\n",
    "    \"Hadoop\": \"<Date> <Time> <Pid> <Level> <Component>: <Content>\",\n",
    "    \"HDFS\": \"<Date> <Time> <Level> <Component>: <Content>\",\n",
    "    \"Spark\": \"<Date> <Time> <Level> <Component>: <Content>\",\n",
    "    \"Zookeeper\": \"<Date> <Time> <Level> <Component>: <Content>\"\n",
    "}\n",
    "\n",
    "# Auto-select format for our dataset\n",
    "log_format = log_format_mappings.get(f\"{config['DATASET_NAME']}\", None)\n",
    "\n",
    "if log_format:\n",
    "    print(f\"‚úÖ Log format for {config['DATASET_NAME']}: {log_format}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No log format found for {config['DATASET_NAME']}. Please update mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5495c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# üß† SEMANTIC LOG ENRICHMENT (Pre-encoding Stage)\n",
    "# ===============================================\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------\n",
    "FIELD_CONFIG = {\n",
    "    \"core_fields\": [\"Component\", \"Level\", \"Method\", \"URL\"],             # Key identifiers\n",
    "    \"enrich_fields\": [\n",
    "        \"ReqID\", \"UserID\", \"TenantID\", \"IP\", \"Status\",\n",
    "        \"ResponseLength\", \"ResponseTime\", \"Service\"                           # Added new enrich fields\n",
    "    ],\n",
    "    \"metadata_fields\": [\n",
    "        \"Component\", \"Level\", \"Pid\", \"ReqID\", \"UserID\",\n",
    "        \"TenantID\", \"IP\", \"Status\", \"Method\", \"URL\",\n",
    "        \"ResponseLength\", \"ResponseTime\", \"Service\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ‚öôÔ∏è CACHED REGEXES (case-insensitive)\n",
    "EXTRA_FIELD_PATTERNS = {\n",
    "    # \"ReqID\": re.compile(r\"(?:req[-_]?id)\\s*[:=]?\\s*\\[?([\\w-]+)\\]?\", re.IGNORECASE),\n",
    "    \"ReqID\": re.compile(r\"\\[req-([\\w-]+)\\b\", re.IGNORECASE),\n",
    "\n",
    "    \"UserID\": re.compile(r\"(?:user[-_]?id)\\s*[:=]\\s*([\\w-]+)\", re.IGNORECASE),\n",
    "    \"TenantID\": re.compile(r\"(?:tenant[-_]?id)\\s*[:=]\\s*([\\w-]+)\", re.IGNORECASE),\n",
    "    \"IP\": re.compile(r\"\\b(\\d{1,3}(?:\\.\\d{1,3}){3})\\b\", re.IGNORECASE),\n",
    "    \"Status\": re.compile(r\"(?:status)\\s*[:=]\\s*(\\d{3})\", re.IGNORECASE),\n",
    "    \"Method\": re.compile(r\"\\b(GET|POST|PUT|DELETE|PATCH|OPTIONS)\\b\", re.IGNORECASE),\n",
    "    \"URL\": re.compile(r\"(https?://[^\\s]+|/[\\w./-]+)\", re.IGNORECASE),\n",
    "    \"ResponseLength\": re.compile(r\"len[:=]\\s*(\\d+)\", re.IGNORECASE),\n",
    "    \"ResponseTime\": re.compile(r\"time[:=]\\s*([\\d\\.]+)\", re.IGNORECASE),\n",
    "}\n",
    "\n",
    "\n",
    "def extract_and_build_metadata(\n",
    "    content: str,\n",
    "    component: str = None,\n",
    "    base_row: Optional[pd.Series] = None,\n",
    "    metadata_fields: Optional[List[str]] = None,\n",
    ") -> Dict[str, str]:\n",
    "    meta = {}\n",
    "\n",
    "    # --- 1Ô∏è‚É£ Regex-based extraction ---\n",
    "    for field, pattern in EXTRA_FIELD_PATTERNS.items():\n",
    "        match = pattern.search(content)\n",
    "        if match:\n",
    "            meta[field.lower()] = match.group(1).strip()\n",
    "\n",
    "    # --- 2Ô∏è‚É£ Merge base_row metadata if provided ---\n",
    "    if base_row is not None and metadata_fields:\n",
    "        for f in metadata_fields:\n",
    "            if f in base_row and pd.notna(base_row[f]):\n",
    "                key = f.lower()\n",
    "                # Avoid overwriting regex-extracted values\n",
    "                if key not in meta:\n",
    "                    meta[key] = base_row[f]\n",
    "\n",
    "    # --- 3Ô∏è‚É£ Timestamp reconstruction ---\n",
    "    date_str = meta.get(\"date\") or (base_row.get(\"Date\") if base_row is not None else \"\")\n",
    "    time_str = meta.get(\"time\") or (base_row.get(\"Time\") if base_row is not None else \"\")\n",
    "    date_str = str(date_str).strip()\n",
    "    time_str = str(time_str).strip()\n",
    "\n",
    "    if date_str and time_str:\n",
    "        ts = pd.to_datetime(f\"{date_str} {time_str}\",\n",
    "                            format=\"%Y-%m-%d %H:%M:%S.%f\",\n",
    "                            errors=\"coerce\")\n",
    "        meta[\"timestamp\"] = ts if pd.notna(ts) else None\n",
    "    else:\n",
    "        meta[\"timestamp\"] = None\n",
    "\n",
    "    # --- 4Ô∏è‚É£ Service shortname ---\n",
    "    if \"service\" not in meta and component:\n",
    "        depth = 2\n",
    "        parts = component.split('.')\n",
    "        # Limit depth to the number of available parts\n",
    "        selected = parts[:min(depth, len(parts))]\n",
    "        service = \".\".join(selected)\n",
    "\n",
    "        meta[\"service\"] = service.replace('_', '-')\n",
    "\n",
    "    # print(content, component)\n",
    "\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeabd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional Metadata Enrichment Functions  ---- OUTDATED\n",
    "\n",
    "# ENRICHMENT_CONFIG = {\n",
    "#     \"core_fields\": [\"Component\", \"Level\", \"Method\", \"URL\"],             # Core log identifiers\n",
    "#     \"enrich_fields\": [\"ReqID\", \"UserID\", \"TenantID\", \"IP\", \"Status\"],   # Enrichment layer\n",
    "#     \"metadata_fields\": [\n",
    "#         \"Component\", \"Level\", \"Pid\", \"ReqID\", \"UserID\", \n",
    "#         \"TenantID\", \"IP\", \"Status\", \"Method\", \"URL\", \n",
    "#         # \"Timestamp\"\n",
    "#     ],  \n",
    "# }\n",
    "\n",
    "# import datetime\n",
    "# import pandas as pd\n",
    "# from typing import List, Dict\n",
    "\n",
    "# # ---------------- Semantic Text Builder ----------------\n",
    "# def build_semantic_text(row: pd.Series, \n",
    "#                         core_fields: List[str], \n",
    "#                         enrich_fields: List[str]) -> str:\n",
    "#     parts = []\n",
    "#     for f in core_fields:\n",
    "#         if f in row and pd.notna(row[f]):\n",
    "#             parts.append(f\"[{row[f]}]\")\n",
    "#     base = \" \".join(parts) + \" \" + str(row.get(\"template\", \"\"))\n",
    "\n",
    "#     extras = []\n",
    "#     for f in enrich_fields:\n",
    "#         if f in row and pd.notna(row[f]):\n",
    "#             extras.append(f\"{f.lower()}={row[f]}\")\n",
    "\n",
    "#     # Return combined text\n",
    "#     return base + (\" \" + \" \".join(extras) if extras else \"\")\n",
    "\n",
    "\n",
    "# # ---------------- Structured Metadata Builder ----------------\n",
    "# def build_structured_metadata(row: pd.Series, \n",
    "#                               metadata_fields: List[str]) -> Dict[str, str]:\n",
    "#     meta = {}\n",
    "#     for f in metadata_fields:\n",
    "#         if f in row and pd.notna(row[f]):\n",
    "#             meta[f.lower()] = row[f]\n",
    "\n",
    "#     # Step 2: Combine Date + Time into Timestamp \n",
    "#     date_str = str(row.get(\"Date\", \"\")).strip()\n",
    "#     time_str = str(row.get(\"Time\", \"\")).strip()\n",
    "\n",
    "#     if date_str and time_str:\n",
    "#         # Use consistent format for speed and reliability\n",
    "#         ts = pd.to_datetime(\n",
    "#             f\"{date_str} {time_str}\",\n",
    "#             format=\"%Y-%m-%d %H:%M:%S.%f\",   # consistent microsecond precision\n",
    "#             errors=\"coerce\"                  # returns NaT if parsing fails\n",
    "#         )\n",
    "#         meta[\"timestamp\"] = ts if pd.notna(ts) else None\n",
    "#     else:\n",
    "#         meta[\"timestamp\"] = None\n",
    "\n",
    "#     return meta\n",
    "\n",
    "\n",
    "# # -------- Apply Enrichment to DataFrame ----------------\n",
    "# def apply_enrichment(df: pd.DataFrame, config: dict):\n",
    "\n",
    "#     print(\"üîß Applying Additional enrichment...\")\n",
    "#     df[\"semantic_text\"] = df.apply(\n",
    "#         lambda r: build_semantic_text(r, config[\"core_fields\"], config[\"enrich_fields\"]), axis=1\n",
    "#     )\n",
    "#     df[\"structured_metadata\"] = df.apply(\n",
    "#         lambda r: build_structured_metadata(r, config[\"metadata_fields\"]), axis=1\n",
    "#     )\n",
    "\n",
    "#     # Extract Timestamp from structured_metadata dict\n",
    "#     df[\"Timestamp\"] = df[\"structured_metadata\"].apply(\n",
    "#         lambda m: m.get(\"timestamp\") if isinstance(m, dict) else None\n",
    "#     )\n",
    "#     # Sort by time for temporal consistency\n",
    "#     df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    \n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae682b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drain_config = TemplateMinerConfig()\n",
    "\n",
    "# --- Core Parameters ---\n",
    "drain_config.profiling_enabled = True\n",
    "# drain_config.drain_sim_th = 0.62         \n",
    "# drain_config.drain_depth = 6\n",
    "drain_config.drain_sim_th = 0.4        \n",
    "drain_config.drain_depth = 5             \n",
    "drain_config.mask_prefix = \"<*>\"         \n",
    "drain_config.extra_delimiters = [\"=\", \",\", \" \", \":\", \"-\", \"\\\"\", \"[\", \"]\", \"(\", \")\"]\n",
    "\n",
    "# --- Dynamic Field Masking ---\n",
    "\n",
    "drain_config.masking_instructions = [\n",
    "        MaskingInstruction(r\"req-[0-9a-f-]+\", \"<REQ_ID>\"),\n",
    "        MaskingInstruction(r\"\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b\", \"<IP>\"),\n",
    "        MaskingInstruction(\n",
    "            # r\"\\[instance:\\s*[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\\]\", \n",
    "            # r\"\\[instance:\\s*([0-9a-fA-F\\-]{36})\\]\",\n",
    "            # r\"\\[instance:\\s*[0-9a-fA-F\\-]+\\]\",\n",
    "            r\"\\[instance:\\s*[0-9a-fA-F0-9\\-]{36}\\]\",\n",
    "            \"<INSTANCE_ID>\"\n",
    "            ),\n",
    "    ]\n",
    "\n",
    "# # --- Initialize TemplateMiner ---\n",
    "# try:\n",
    "#     template_miner = TemplateMiner(persistence, drain_config)\n",
    "#     print(\"‚úÖ Drain3 TemplateMiner initialized successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è Error initializing TemplateMiner:\", e)\n",
    "#     raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99236cab",
   "metadata": {},
   "source": [
    "### MetaDataDrainParser Class - Custom class that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c75f3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetadataDrainParser:\n",
    "    def __init__(self, log_format: str, \n",
    "                 structured_csv: str, save_every: int, templates_csv: str,\n",
    "                 field_config: dict, \n",
    "                 mode: str,\n",
    "                 ):\n",
    "        # self.template_miner = template_miner\n",
    "        self.log_format = log_format\n",
    "        self.structured_csv = structured_csv\n",
    "        self.save_every = save_every\n",
    "        self.templates_csv = templates_csv\n",
    "        self.field_config = field_config or FIELD_CONFIG\n",
    "        self.mode = mode.lower()\n",
    "\n",
    "        self._init_template_miner()\n",
    "\n",
    "        self.buffer = []\n",
    "        self.total = 0\n",
    "        self.unique_templates = set()\n",
    "\n",
    "        # Clean up existing files\n",
    "\n",
    "        for path in [self.structured_csv, self.templates_csv]:\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "                    print(f\"üóëÔ∏è Removed existing {path} for a fresh run.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not remove {path}: {e}\")\n",
    "\n",
    "    def _init_template_miner(self):\n",
    "        \n",
    "        if self.mode == \"fresh\":\n",
    "            print(f\"üßπ Starting fresh parse (ignoring old state).\")\n",
    "            persist = None\n",
    "        elif self.mode == \"incremental\":\n",
    "            print(f\"‚ôªÔ∏è Loading existing state from {persist} (incremental mode).\")\n",
    "            persist = FilePersistence(persistence)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'fresh' or 'incremental'\")\n",
    "\n",
    "        self.template_miner = TemplateMiner(persist, drain_config)\n",
    "        print(\"‚úÖ Drain3 TemplateMiner initialized successfully.\")\n",
    "\n",
    "\n",
    "    def detect_log_format(self, sample_line: str, base_format: str) -> str:\n",
    "\n",
    "        rotated_pattern = r'^[\\w\\-.]+\\.log(?:\\.\\d+)?\\.\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2}'\n",
    "        if re.match(rotated_pattern, sample_line):\n",
    "            if not base_format.startswith(\"<File>\"):\n",
    "                print(\"üß† Auto-detected rotated log prefix ‚Äî prepending <File> to log format.\")\n",
    "                return \"<File> \" + base_format\n",
    "        return base_format\n",
    "\n",
    "    def process_line(self, raw_line: str, line_no: int):\n",
    "        \"\"\"Parse one log line, enrich with metadata, and append to buffer.\"\"\"\n",
    "        raw = raw_line.rstrip(\"\\n\")\n",
    "        \n",
    "        if self.detect_log_format and line_no == 1:\n",
    "            old_format = self.log_format\n",
    "            self.log_format = self.detect_log_format(raw, old_format)\n",
    "            if self.log_format != old_format:\n",
    "                print(f\"‚úÖ Adjusted log format ‚Üí {self.log_format}\")\n",
    "\n",
    "        # ------- Extract Structured metadata\n",
    "        parsed_meta = parse_line_with_format(raw, self.log_format)\n",
    "        content = parsed_meta.get(\"Content\") or raw \n",
    "        # content = parsed_meta.get(\"Content\", \"\")\n",
    "        component = parsed_meta.get(\"Component\")\n",
    "\n",
    "        # ------- Extract additional metadata fields \n",
    "        unified_meta = extract_and_build_metadata(\n",
    "            content,\n",
    "            component,\n",
    "            base_row=pd.Series(parsed_meta),\n",
    "            metadata_fields=FIELD_CONFIG[\"metadata_fields\"],\n",
    "        )\n",
    "\n",
    "        # -------- Send to Drain3 for template extraction\n",
    "        try:\n",
    "            result = self.template_miner.add_log_message(content)\n",
    "        except Exception as e:\n",
    "            result = {\"cluster_id\": None, \"template_mined\": None, \"change_type\": f\"error:{e}\"}\n",
    "\n",
    "        # --------- Collect Drain3 + metadata output\n",
    "        template = result.get(\"template_mined\")\n",
    "        template_id = result.get(\"cluster_id\")\n",
    "\n",
    "        self.unique_templates.add(template or f\"__none_{template_id}\")\n",
    "        \n",
    "        row = {\n",
    "            \"line_no\": line_no,\n",
    "            \"raw\": raw,\n",
    "            \"content\": content,\n",
    "            \"template_id\": template_id,\n",
    "            \"template\": template,\n",
    "        }\n",
    "\n",
    "        row.update(unified_meta)\n",
    "        # print(row)\n",
    "        self.buffer.append(row)\n",
    "        self.total += 1\n",
    "\n",
    "        # Periodic flush - DRAIN\n",
    "        if len(self.buffer) >= self.save_every:\n",
    "            self.flush_to_csv()\n",
    "\n",
    "                \n",
    "    def flush_to_csv(self):\n",
    "        df = pd.DataFrame(self.buffer)\n",
    "        header = not os.path.exists(self.structured_csv)\n",
    "        df.to_csv(self.structured_csv, mode=\"a\", index=False, header=header)\n",
    "        print(f\"[flush] wrote {len(self.buffer)} rows ‚Üí {self.structured_csv} (total parsed {self.total})\")\n",
    "        self.buffer = []\n",
    "\n",
    "    # ----- EXPORTING THE DRAIN TEMPLATES\n",
    "\n",
    "    def export_templates(self):\n",
    "\n",
    "        # --- GET ALL DRAIN CLUSTERS\n",
    "        clusters = self.template_miner.drain.clusters\n",
    "        if not clusters:\n",
    "            print(\"‚ö†Ô∏è No templates discovered yet ‚Äî skipping export.\")\n",
    "            return\n",
    "\n",
    "        # --- Normalize clusters iterable ---\n",
    "        if isinstance(clusters, dict):\n",
    "            cluster_list = clusters.values()\n",
    "        else:\n",
    "            cluster_list = clusters \n",
    "\n",
    "        templates = []\n",
    "        for cluster in cluster_list:\n",
    "            cid = getattr(cluster, \"cluster_id\", None)\n",
    "\n",
    "            templates.append({\n",
    "                \"template_id\": cid,\n",
    "                \"template\": cluster.get_template(),\n",
    "                \"occurrences\": getattr(cluster, \"size\", None),\n",
    "            })\n",
    "        df = pd.DataFrame(templates)\n",
    "        header = not os.path.exists(self.templates_csv)\n",
    "        df.to_csv(self.templates_csv, mode=\"w\" if header else \"a\", index=False, header=header)\n",
    "\n",
    "    def finalize(self):\n",
    "        # ----- Final flush after finishing all lines.\n",
    "        if self.buffer:\n",
    "            self.flush_to_csv()\n",
    "        print(f\"‚úÖ Parsing complete. Total parsed lines: {self.total}\")\n",
    "        print(f\"üß© Unique templates discovered: {len(self.unique_templates)}\")\n",
    "        # ----- Export template catalogue\n",
    "        self.export_templates()\n",
    "        print(f\"üìä Template catalogue exported for validation and benchmarking.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5d7d7",
   "metadata": {},
   "source": [
    "### Initialize and Configure Drain3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878348ce",
   "metadata": {},
   "source": [
    "#### Running the Log Parsing and Metadata Extraction Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58797ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Starting fresh parse (ignoring old state).\n",
      "‚úÖ Drain3 TemplateMiner initialized successfully.\n",
      "üóëÔ∏è Removed existing data/OpenStack_2k.log_structured.csv for a fresh run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 210it [00:00, 2034.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Auto-detected rotated log prefix ‚Äî prepending <File> to log format.\n",
      "‚úÖ Adjusted log format ‚Üí <File> <Date> <Time> <Pid> <Level> <Component> <Content>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 2000it [00:00, 4138.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flush] wrote 2000 rows ‚Üí data/OpenStack_2k.log_structured.csv (total parsed 2000)\n",
      "‚úÖ Parsing complete. Total parsed lines: 2000\n",
      "üß© Unique templates discovered: 45\n",
      "üìä Template catalogue exported for validation and benchmarking.\n"
     ]
    }
   ],
   "source": [
    "parser = MetadataDrainParser(\n",
    "    # template_miner=template_miner,\n",
    "    log_format=log_format,\n",
    "    field_config=FIELD_CONFIG,\n",
    "    structured_csv=OUTPUT_CSV,\n",
    "    templates_csv=TEMPLATES_CSV,\n",
    "    mode=\"fresh\",\n",
    "    # save_every=500,\n",
    "    save_every=50000,\n",
    ")\n",
    "\n",
    "with open(DATASET_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "    for i, line in enumerate(tqdm(fh, desc=\"Parsing lines\"), start=1):\n",
    "        if config['MAX_LINES'] and i > config['MAX_LINES']:\n",
    "            break\n",
    "        parser.process_line(line, i)\n",
    "\n",
    "# Finalize and save\n",
    "parser.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEWING THE DRAIN CLUSTERS CREATED\n",
    "\n",
    "# for cluster in template_miner.drain.clusters:\n",
    "#     print(f\"Template: {cluster.get_template()}\")\n",
    "#     print(f\"Occurrences: {cluster.size}\")\n",
    "#     print(f\"Cluster: {cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd49051",
   "metadata": {},
   "source": [
    "#### DATA CLEANING AND DEDUPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cc67517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_template(text, preserve_symbols=\":=0123456789\"):\n",
    "    # REMOVE ALL PLACEHOLDER PATTERNS\n",
    "    text = re.sub(r\"<\\*>\", \"\", text)\n",
    "    # NORMALIZE UUIDS\n",
    "    text = re.sub(r'\\b[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\\b', \"<*>\", text)\n",
    "    # NORMALIZE OTHER ALPHANUMERIC IDS\n",
    "    text = re.sub(r'\\b[0-9a-f]{20,}\\b', \"<*>\", text)\n",
    "    # REPLACE NUMBERS\n",
    "    # text = re.sub(r'\\b\\d+\\b', \"<*>\", text)\n",
    "    # REMOVE ALL UNWANTED SYMBOLS EXCEPT THE ALLOWED SYMBOLS\n",
    "    pattern = rf\"[^\\w\\s{re.escape(preserve_symbols)}/.-]\"\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    # NORMALIZE ALL WHITESPACES\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned & grouped templates saved: data/OpenStack_2k.log_cleaned_templates.csv\n",
      "Number of unique semantic templates: 21\n",
      "                                        semantic_text  template_ids  \\\n",
      "4                                    req_id - - - - -   [2, 10, 16]   \n",
      "16                           req_id - - - instance_id  [13, 14, 19]   \n",
      "0   - instance_id during sync_power_state the inst...          [25]   \n",
      "11  req_id - - - - - the instance sync for host cp...          [23]   \n",
      "19  req_id - - - instance_id took seconds to deall...          [15]   \n",
      "18  req_id - - - instance_id took seconds to build...           [9]   \n",
      "17  req_id - - - instance_id attempting claim: mem...          [18]   \n",
      "15           req_id - - - http/1.1 status: len: time:           [1]   \n",
      "14  req_id - - - http exception thrown: no instanc...          [17]   \n",
      "13           req_id - - - creating event for instance           [6]   \n",
      "12  req_id - - - - - while synchronizing instance ...          [24]   \n",
      "10  req_id - - - - - running instance usage audit ...          [21]   \n",
      "1                - instance_id instance successfully.           [7]   \n",
      "9   req_id - - - - - instance_id during sync_power...           [3]   \n",
      "8   req_id - - - - - image at /var/lib/nova/instan...           [4]   \n",
      "7   req_id - - - - - final resource view: name=cp-...          [11]   \n",
      "6   req_id - - - - - base or swap file too young t...          [22]   \n",
      "5   req_id - - - - - base /var/lib/nova/instances/...           [5]   \n",
      "3              - ipip get http/1.1 status: len: time:          [12]   \n",
      "2            - instance_id vm stopped lifecycle event          [20]   \n",
      "20  req_id - - - instance_id took seconds to the i...           [8]   \n",
      "\n",
      "    occurrences  \n",
      "4             3  \n",
      "16            3  \n",
      "0             1  \n",
      "11            1  \n",
      "19            1  \n",
      "18            1  \n",
      "17            1  \n",
      "15            1  \n",
      "14            1  \n",
      "13            1  \n",
      "12            1  \n",
      "10            1  \n",
      "1             1  \n",
      "9             1  \n",
      "8             1  \n",
      "7             1  \n",
      "6             1  \n",
      "5             1  \n",
      "3             1  \n",
      "2             1  \n",
      "20            1  \n"
     ]
    }
   ],
   "source": [
    "os.makedirs(config[\"DATA_DIR\"], exist_ok=True)\n",
    "\n",
    "# CLEANING THE TEMPLATES CSV\n",
    "\n",
    "def clean_and_group_templates(templates_csv: str, output_csv: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean template texts, deduplicate by semantic content,\n",
    "    and preserve all associated template_ids in a list.\n",
    "    \"\"\"\n",
    "    templates_df = pd.read_csv(templates_csv)\n",
    "\n",
    "    # Clean the templates\n",
    "    templates_df['semantic_text'] = templates_df['template'].apply(clean_template)\n",
    "\n",
    "    # Count occurrences\n",
    "    templates_df['occurrences'] = templates_df.groupby('semantic_text')['semantic_text'].transform('count')\n",
    "\n",
    "    # Group by the cleaned semantic_text\n",
    "    grouped_df = templates_df.groupby('semantic_text').agg(\n",
    "        template_ids=('template_id', lambda x: list(x)),\n",
    "        occurrences=('occurrences', 'max')  # keep the max occurrences\n",
    "    ).reset_index()\n",
    "\n",
    "    # Optionally sort by occurrences descending\n",
    "    grouped_df = grouped_df.sort_values('occurrences', ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    grouped_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Cleaned & grouped templates saved: {output_csv}\")\n",
    "    print(f\"Number of unique semantic templates: {len(grouped_df)}\")\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "grouped_templates = clean_and_group_templates(TEMPLATES_CSV, CLEANED_CSV)\n",
    "print(grouped_templates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
