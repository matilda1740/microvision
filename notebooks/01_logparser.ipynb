{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6a98f6",
   "metadata": {},
   "source": [
    "## MicroVision\n",
    "\n",
    "This notebook covers the following: \n",
    "\n",
    "1. Loading and Preprocessing the raw log datasets\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b0408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd() == '/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision/notebooks':\n",
    "    os.chdir('/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision')\n",
    "    print(\"Changed!!\")\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306533c9",
   "metadata": {},
   "source": [
    "#### Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc4c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install drain3 pandas matplotlib tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a9cba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Drain3 and dependencies imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.file_persistence import FilePersistence\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.template_miner_config import MaskingInstruction\n",
    "\n",
    "print(\"‚úÖ Drain3 and dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e97fe7",
   "metadata": {},
   "source": [
    "#### Configurations and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3740314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: data/OpenStack_full.log\n",
      "The extracted templated will be written to: data/OpenStack_full.log_templates.csv\n",
      "The extracted templates enriched with metadata will be written to: data/OpenStack_full.log_enriched.csv\n",
      "Persistence path (Drain3) data/persistence/drain3_state\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"DATA_DIR\": \"data\",\n",
    "        \"DATASET_NAME\": \"OpenStack\",\n",
    "        \"DATASET_LOG\": \"_full.log\",\n",
    "        \"ENRICHED_CSV\": \"_enriched.csv\",\n",
    "        \"TEMPLATES_CSV\": \"_templates.csv\",\n",
    "        \"TEMPLATES_JSON\": \"_templates.json\",\n",
    "        \"MAX_LINES\": None,  # Set to None to process all lines\n",
    "        \"PERSISTENCE_PATH\": \"persistence\",\n",
    "        \"DRAIN_PATH\": \"drain3_state\",\n",
    "}\n",
    "\n",
    "\n",
    "DATASET_PATH = f\"{config['DATA_DIR']}/{config['DATASET_NAME']+config['DATASET_LOG']}\"\n",
    "\n",
    "\n",
    "ENRICHED_CSV = f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['ENRICHED_CSV']}\"\n",
    "TEMPLATES_CSV = f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['TEMPLATES_CSV']}\"\n",
    "TEMPLATES_JSON = f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['TEMPLATES_JSON']}\"\n",
    "\n",
    "persistence_dir = os.path.join(f\"{config['DATA_DIR']}/{config['PERSISTENCE_PATH']}\", config['DRAIN_PATH'])\n",
    "os.makedirs(persistence_dir, exist_ok=True)\n",
    "persistence = FilePersistence(f\"{persistence_dir}/drain3_state.bin\")\n",
    "\n",
    "print(\"Dataset Path:\", DATASET_PATH)\n",
    "print(\"The extracted templated will be written to:\", TEMPLATES_CSV)\n",
    "print(\"The extracted templates enriched with metadata will be written to:\", ENRICHED_CSV)\n",
    "print(\"Persistence path (Drain3)\", f\"{persistence_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8bedae",
   "metadata": {},
   "source": [
    "### Utility Functions for Log Parsing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd2b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "# ------- UF: Loading Raw Log files\n",
    "\n",
    "def load_raw_logs(log_dir: str, dataset_name: str):\n",
    "    log_files = glob.glob(f\"{log_dir}/{dataset_name}*.log\")\n",
    "    raw_logs = []\n",
    "    for file_path in log_files:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            raw_logs.extend(f.readlines())\n",
    "    return raw_logs\n",
    "\n",
    "\n",
    "# ------- UF: Convert LogPai-style format to regex\n",
    "\n",
    "def log_format_to_regex(log_format: str) -> str:\n",
    "    tokens = re.findall(r\"<([^>]+)>\", log_format)\n",
    "    regex = re.escape(log_format)\n",
    "    for t in tokens:\n",
    "        esc = re.escape(f\"<{t}>\")\n",
    "        if t.lower() == \"content\":\n",
    "            repl = rf\"(?P<{t}>.*)\"\n",
    "        else:\n",
    "            repl = rf\"(?P<{t}>.+?)\"\n",
    "        regex = regex.replace(esc, repl, 1)\n",
    "    regex = regex.replace(r\"\\ \", r\"\\s+\")\n",
    "    return rf\"^{regex}$\"\n",
    "\n",
    "\n",
    "# ------- UF: Parse log line with given format \n",
    "\n",
    "def parse_line_with_format(line: str, log_format: str):\n",
    "    \"\"\"Return dict of matched groups (or {'Content': line} fallback).\"\"\"\n",
    "    if not log_format:\n",
    "        return {\"Content\": line}\n",
    "    regex = log_format_to_regex(log_format)\n",
    "    line = line.strip()\n",
    "    m = re.match(regex, line)\n",
    "    if not m:\n",
    "        return {\"ParseError\": True, \"Raw\": line, \"Content\": line}\n",
    "    return m.groupdict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a9d8e",
   "metadata": {},
   "source": [
    "### Dynamically Select Log Format Mapping for current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a529a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log format for OpenStack: <Date> <Time> <Pid> <Level> <Component> <Content>\n"
     ]
    }
   ],
   "source": [
    "# LOG FORMAT MAPPINGS FROM LogPai's GitHub\n",
    "log_format_mappings = {\n",
    "    \"OpenStack\": \"<Date> <Time> <Pid> <Level> <Component> <Content>\",\n",
    "    \"Hadoop\": \"<Date> <Time> <Pid> <Level> <Component>: <Content>\",\n",
    "    \"HDFS\": \"<Date> <Time> <Level> <Component>: <Content>\",\n",
    "    \"Spark\": \"<Date> <Time> <Level> <Component>: <Content>\",\n",
    "    \"Zookeeper\": \"<Date> <Time> <Level> <Component>: <Content>\"\n",
    "}\n",
    "\n",
    "# Auto-select format for our dataset\n",
    "log_format = log_format_mappings.get(f\"{config['DATASET_NAME']}\", None)\n",
    "\n",
    "if log_format:\n",
    "    print(f\"‚úÖ Log format for {config['DATASET_NAME']}: {log_format}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No log format found for {config['DATASET_NAME']}. Please update mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aeabd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Metadata Enrichment Functions\n",
    "\n",
    "ENRICHMENT_CONFIG = {\n",
    "    \"core_fields\": [\"Component\", \"Level\", \"Method\", \"URL\"],             # Core log identifiers\n",
    "    \"enrich_fields\": [\"ReqID\", \"UserID\", \"TenantID\", \"IP\", \"Status\"],   # Enrichment layer\n",
    "    \"metadata_fields\": [\n",
    "        \"Component\", \"Level\", \"Pid\", \"ReqID\", \"UserID\", \n",
    "        \"TenantID\", \"IP\", \"Status\", \"Method\", \"URL\", \n",
    "        # \"Timestamp\"\n",
    "    ],  \n",
    "}\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# ---------------- Semantic Text Builder ----------------\n",
    "def build_semantic_text(row: pd.Series, \n",
    "                        core_fields: List[str], \n",
    "                        enrich_fields: List[str]) -> str:\n",
    "    parts = []\n",
    "    for f in core_fields:\n",
    "        if f in row and pd.notna(row[f]):\n",
    "            parts.append(f\"[{row[f]}]\")\n",
    "    base = \" \".join(parts) + \" \" + str(row.get(\"template\", \"\"))\n",
    "\n",
    "    extras = []\n",
    "    for f in enrich_fields:\n",
    "        if f in row and pd.notna(row[f]):\n",
    "            extras.append(f\"{f.lower()}={row[f]}\")\n",
    "\n",
    "    # Return combined text\n",
    "    return base + (\" \" + \" \".join(extras) if extras else \"\")\n",
    "\n",
    "\n",
    "# ---------------- Structured Metadata Builder ----------------\n",
    "def build_structured_metadata(row: pd.Series, \n",
    "                              metadata_fields: List[str]) -> Dict[str, str]:\n",
    "    meta = {}\n",
    "    for f in metadata_fields:\n",
    "        if f in row and pd.notna(row[f]):\n",
    "            meta[f.lower()] = row[f]\n",
    "\n",
    "    # Step 2: Combine Date + Time into Timestamp \n",
    "    date_str = str(row.get(\"Date\", \"\")).strip()\n",
    "    time_str = str(row.get(\"Time\", \"\")).strip()\n",
    "\n",
    "    if date_str and time_str:\n",
    "        # Use consistent format for speed and reliability\n",
    "        ts = pd.to_datetime(\n",
    "            f\"{date_str} {time_str}\",\n",
    "            format=\"%Y-%m-%d %H:%M:%S.%f\",   # consistent microsecond precision\n",
    "            errors=\"coerce\"                  # returns NaT if parsing fails\n",
    "        )\n",
    "        meta[\"timestamp\"] = ts if pd.notna(ts) else None\n",
    "    else:\n",
    "        meta[\"timestamp\"] = None\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "# -------- Apply Enrichment to DataFrame ----------------\n",
    "def apply_enrichment(df: pd.DataFrame, config: dict):\n",
    "\n",
    "    print(\"üîß Applying Additional enrichment...\")\n",
    "    df[\"semantic_text\"] = df.apply(\n",
    "        lambda r: build_semantic_text(r, config[\"core_fields\"], config[\"enrich_fields\"]), axis=1\n",
    "    )\n",
    "    df[\"structured_metadata\"] = df.apply(\n",
    "        lambda r: build_structured_metadata(r, config[\"metadata_fields\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Extract Timestamp from structured_metadata dict\n",
    "    df[\"Timestamp\"] = df[\"structured_metadata\"].apply(\n",
    "        lambda m: m.get(\"timestamp\") if isinstance(m, dict) else None\n",
    "    )\n",
    "    # Sort by time for temporal consistency\n",
    "    df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99236cab",
   "metadata": {},
   "source": [
    "### MetaDataDrainParser Class - Custom class that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75f3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetadataDrainParser:\n",
    "    def __init__(self, template_miner, log_format: str, \n",
    "                 enriched_csv: str, save_every: int, templates_csv: str, templates_json: str, \n",
    "                 enrichment_config: dict, \n",
    "                 ):\n",
    "        self.template_miner = template_miner\n",
    "        self.log_format = log_format\n",
    "        self.enriched_csv = enriched_csv\n",
    "        self.save_every = save_every\n",
    "        self.templates_csv = templates_csv\n",
    "        self.templates_json = templates_json\n",
    "        self.enrichment_config = enrichment_config or ENRICHMENT_CONFIG\n",
    "\n",
    "        self.buffer = []\n",
    "        self.total = 0\n",
    "        self.unique_templates = set()\n",
    "\n",
    "        # Clean up existing files\n",
    "\n",
    "        for path in [self.enriched_csv, self.templates_csv, self.templates_json]:\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "                    print(f\"üóëÔ∏è Removed existing {path} for a fresh run.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not remove {path}: {e}\")\n",
    "\n",
    "    def detect_log_format(self, sample_line: str, base_format: str) -> str:\n",
    "\n",
    "        rotated_pattern = r'^[\\w\\-.]+\\.log(?:\\.\\d+)?\\.\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2}'\n",
    "        if re.match(rotated_pattern, sample_line):\n",
    "            if not base_format.startswith(\"<File>\"):\n",
    "                print(\"üß† Auto-detected rotated log prefix ‚Äî prepending <File> to log format.\")\n",
    "                return \"<File> \" + base_format\n",
    "        return base_format\n",
    "\n",
    "    def process_line(self, raw_line: str, line_no: int):\n",
    "        \"\"\"Parse one log line, enrich with metadata, and append to buffer.\"\"\"\n",
    "        raw = raw_line.rstrip(\"\\n\")\n",
    "        \n",
    "        if self.detect_log_format and line_no == 1:\n",
    "            old_format = self.log_format\n",
    "            self.log_format = self.detect_log_format(raw, old_format)\n",
    "            if self.log_format != old_format:\n",
    "                print(f\"‚úÖ Adjusted log format ‚Üí {self.log_format}\")\n",
    "\n",
    "        # ------- Extract structured metadata\n",
    "        parsed_meta = parse_line_with_format(raw, self.log_format)\n",
    "        content = parsed_meta.get(\"Content\") or raw\n",
    "\n",
    "        # -------- Send to Drain3 for template extraction\n",
    "        try:\n",
    "            result = self.template_miner.add_log_message(content)\n",
    "        except Exception as e:\n",
    "            result = {\"cluster_id\": None, \"template_mined\": None, \"change_type\": f\"error:{e}\"}\n",
    "\n",
    "        # --------- Collect Drain3 + metadata output\n",
    "        template = result.get(\"template_mined\")\n",
    "        template_id = result.get(\"cluster_id\")\n",
    "\n",
    "        self.unique_templates.add(template or f\"__none_{template_id}\")\n",
    "\n",
    "        # Merge all information\n",
    "        row = {\n",
    "            \"line_no\": line_no,\n",
    "            \"raw\": raw,\n",
    "            \"content\": content,\n",
    "            \"template_id\": template_id,\n",
    "            \"template\": template,\n",
    "        }\n",
    "        # Add extracted metadata fields (timestamp, service, level, etc.)\n",
    "        row.update(parsed_meta)\n",
    "\n",
    "        self.buffer.append(row)\n",
    "        self.total += 1\n",
    "\n",
    "        # Periodic flush\n",
    "        if len(self.buffer) >= self.save_every:\n",
    "            self.flush_to_csv()\n",
    "\n",
    "    def flush_to_csv(self):\n",
    "        \"\"\"Write buffer to disk and clear memory.\"\"\"\n",
    "        df = pd.DataFrame(self.buffer)\n",
    "        df = apply_enrichment(df, self.enrichment_config)\n",
    "\n",
    "        header = not os.path.exists(self.enriched_csv)\n",
    "        df.to_csv(self.enriched_csv, mode=\"a\", index=False, header=header)\n",
    "        print(f\"[flush] wrote {len(self.buffer)} rows ‚Üí {self.enriched_csv} (total parsed {self.total})\")\n",
    "        self.buffer = []\n",
    "\n",
    "    def export_templates(self):\n",
    "        \"\"\"Extract and persist Drain3 templates \"\"\"\n",
    "        clusters = self.template_miner.drain.clusters\n",
    "        records = []\n",
    "        for c in clusters:\n",
    "            tmpl = (getattr(c, \"template\", None)\n",
    "                    or (lambda f: f() if callable(f) else None)(getattr(c, \"get_template\", None))\n",
    "                    or getattr(c, \"template_str\", None)\n",
    "                    or getattr(c, \"example_log\", None))\n",
    "            \n",
    "            records.append({\n",
    "                \"template_id\": c.cluster_id,\n",
    "                \"template\": tmpl,\n",
    "                \"size\": c.size,\n",
    "            })\n",
    "\n",
    "        # Export to CSV\n",
    "        df = pd.DataFrame(records)\n",
    "        df.to_csv(self.templates_csv, index=False)\n",
    "        print(f\"üß© Exported {len(records)} templates ‚Üí {self.templates_csv}\")\n",
    "\n",
    "        # Export to JSON for downstream semantic loading\n",
    "        with open(self.templates_json, \"w\") as f:\n",
    "            json.dump(records, f, indent=2)\n",
    "        print(f\"üì¶ Templates also saved as JSON ‚Üí {self.templates_json}\")\n",
    "\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Final flush after finishing all lines.\"\"\"\n",
    "        if self.buffer:\n",
    "            self.flush_to_csv()\n",
    "        print(f\"‚úÖ Parsing complete. Total parsed lines: {self.total}\")\n",
    "        print(f\"üß© Unique templates discovered: {len(self.unique_templates)}\")\n",
    "        \n",
    "        # Export template catalogue\n",
    "        self.export_templates()\n",
    "        print(\"üìä Template catalogue exported for validation and benchmarking.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5d7d7",
   "metadata": {},
   "source": [
    "### Initialize and Configure Drain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae682b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Drain3 TemplateMiner initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "drain_config = TemplateMinerConfig()\n",
    "\n",
    "# --- Core Parameters ---\n",
    "drain_config.profiling_enabled = True\n",
    "drain_config.drain_sim_th = 0.45         \n",
    "drain_config.drain_depth = 5             \n",
    "drain_config.mask_prefix = \"<*>\"         \n",
    "drain_config.extra_delimiters = [\"=\", \",\", \" \", \":\", \"-\"]\n",
    "\n",
    "# --- Dynamic Field Masking ---\n",
    "\n",
    "drain_config.masking_instructions = [\n",
    "        MaskingInstruction(r\"req-[0-9a-f-]+\", \"<REQ_ID>\"),\n",
    "        MaskingInstruction(r\"[0-9a-f]{32}\", \"<HASH>\"),\n",
    "        MaskingInstruction(r\"\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b\", \"<IP>\"),\n",
    "    ]\n",
    "\n",
    "# --- Initialize TemplateMiner ---\n",
    "try:\n",
    "    template_miner = TemplateMiner(persistence, drain_config)\n",
    "    print(\"‚úÖ Drain3 TemplateMiner initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Error initializing TemplateMiner:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878348ce",
   "metadata": {},
   "source": [
    "#### Running the Log Parsing and Metadata Extraction Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58797ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Auto-detected rotated log prefix ‚Äî prepending <File> to log format.\n",
      "‚úÖ Adjusted log format ‚Üí <File> <Date> <Time> <Pid> <Level> <Component> <Content>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 49918it [00:01, 28362.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying Additional enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 52755it [00:07, 1703.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flush] wrote 50000 rows ‚Üí data/OpenStack_full.log_enriched.csv (total parsed 50000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 98030it [00:08, 27535.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying Additional enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 103863it [00:13, 2556.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flush] wrote 50000 rows ‚Üí data/OpenStack_full.log_enriched.csv (total parsed 100000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 149755it [00:15, 28105.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying Additional enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 155598it [00:20, 2523.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flush] wrote 50000 rows ‚Üí data/OpenStack_full.log_enriched.csv (total parsed 150000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 198334it [00:22, 27810.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying Additional enrichment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 204225it [00:27, 2538.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flush] wrote 50000 rows ‚Üí data/OpenStack_full.log_enriched.csv (total parsed 200000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lines: 207632it [00:27, 7516.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying Additional enrichment...\n",
      "[flush] wrote 7632 rows ‚Üí data/OpenStack_full.log_enriched.csv (total parsed 207632)\n",
      "‚úÖ Parsing complete. Total parsed lines: 207632\n",
      "üß© Unique templates discovered: 31\n",
      "üß© Exported 31 templates ‚Üí data/OpenStack_full.log_templates.csv\n",
      "üì¶ Templates also saved as JSON ‚Üí data/OpenStack_full.log_templates.json\n",
      "üìä Template catalogue exported for validation and benchmarking.\n"
     ]
    }
   ],
   "source": [
    "parser = MetadataDrainParser(\n",
    "    template_miner=template_miner,\n",
    "    log_format=log_format,\n",
    "    enriched_csv=ENRICHED_CSV,\n",
    "    save_every=50000,\n",
    "    templates_csv=TEMPLATES_CSV,\n",
    "    templates_json=TEMPLATES_JSON,\n",
    "    enrichment_config=ENRICHMENT_CONFIG,\n",
    ")\n",
    "\n",
    "with open(DATASET_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "    for i, line in enumerate(tqdm(fh, desc=\"Parsing lines\"), start=1):\n",
    "        if config['MAX_LINES'] and i > config['MAX_LINES']:\n",
    "            break\n",
    "        parser.process_line(line, i)\n",
    "\n",
    "# Finalize and save\n",
    "parser.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cde957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
