{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c329d67",
   "metadata": {},
   "source": [
    "### MicroVision \n",
    "\n",
    "## Log Template Enrichment Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd() == '/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision/notebooks':\n",
    "    os.chdir('/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision')\n",
    "    print(\"Changed!!\")\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas tqdm  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a670c",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess templates for semantic embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3014ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "config = {\n",
    "        \"DATA_DIR\": \"data\",\n",
    "        \"DATASET_NAME\": \"OpenStack\",\n",
    "        \"OUTPUT_SUFFIX\": \"_enriched.csv\",\n",
    "        \"TEMPLATE_COL\": \"template\",\n",
    "        # \"KNOWN_SERVICES\": \"known_services.json\"\n",
    "    }\n",
    "\n",
    "os.makedirs(config[\"DATA_DIR\"], exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Data directory: {config['DATA_DIR']}\")\n",
    "print(f\"üìÇ Dataset Name: {config['DATASET_NAME']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions that facilitate the Enrichment Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- UF1: CLEANING FUNCTION -------\n",
    "def clean_template(text, preserve_symbols=\":=\"):\n",
    "    pattern = rf\"[^\\w\\s{re.escape(preserve_symbols)}/.-]\"\n",
    "    text = re.sub(r\"<\\*>\", \"\", text)\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39323e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_service_hint(template, known_services=None):\n",
    "    \"\"\"\n",
    "    Extracts the most likely service/component name from a Drain3 log template.\n",
    "    Designed for use in distributed system logs (OpenStack, Hadoop, Spark, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    text = template.lower()\n",
    "\n",
    "    # 1Ô∏è‚É£ Tier 1: Check known service names first (from known_services.json)\n",
    "    if known_services:\n",
    "        matches = [svc for svc in known_services if re.search(rf\"\\b{svc}\\b\", text)]\n",
    "        if matches:\n",
    "            # Prefer the longest (most specific) match\n",
    "            return sorted(matches, key=len, reverse=True)[0]\n",
    "\n",
    "    # 2Ô∏è‚É£ Tier 2: Common distributed system service identifiers\n",
    "    system_patterns = [\n",
    "        r\"(?P<service>nova[-_]api|nova[-_]compute|nova[-_]scheduler|neutron[-_]agent|cinder[-_]volume|glance|keystone)\",\n",
    "        r\"(?P<service>namenode|datanode|hdfs|yarn|mapreduce|spark[-_]driver|spark[-_]executor|flink|zookeeper|kafka|hbase)\",\n",
    "    ]\n",
    "    for p in system_patterns:\n",
    "        m = re.search(p, text)\n",
    "        if m:\n",
    "            return m.group(\"service\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Tier 3: Log filename or structured marker\n",
    "    structured_patterns = [\n",
    "        r\"(?P<service>[a-z0-9_-]+)\\.log\",\n",
    "        r\"\\[(?P<service>[a-z0-9_-]+)\\]\",\n",
    "        r\"\\bmodule[:=]\\s*(?P<service>[a-z0-9_-]+)\\b\",\n",
    "        r\"\\bcomponent[:=]\\s*(?P<service>[a-z0-9_-]+)\\b\",\n",
    "    ]\n",
    "    for p in structured_patterns:\n",
    "        m = re.search(p, text)\n",
    "        if m:\n",
    "            return m.group(\"service\")\n",
    "\n",
    "    # 4Ô∏è‚É£ Tier 4: Token heuristics (fallback)\n",
    "    tokens = re.findall(r\"[a-z0-9_-]+\", text)\n",
    "    if tokens:\n",
    "        # Prefer tokens containing known service-like substrings\n",
    "        for t in tokens:\n",
    "            if any(keyword in t for keyword in [\"api\", \"compute\", \"agent\", \"service\", \"scheduler\", \"controller\"]):\n",
    "                return t\n",
    "        # Avoid generic or numeric tokens\n",
    "        meaningful_tokens = [t for t in tokens if len(t) > 3 and not re.match(r\"^[0-9a-f]{8,}$\", t)]\n",
    "        if meaningful_tokens:\n",
    "            return meaningful_tokens[0]\n",
    "\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- UF3: TIMESTAMP EXTRACTION FUNCTION ------- -> Moved to Future Plans\n",
    "\n",
    "# def extract_timestamp(template):\n",
    "#     match = re.search(r\"\\d{4}-\\d{2}-\\d{2}[\\sT]\\d{2}:\\d{2}:\\d{2}\", template)\n",
    "#     return match.group(0) if match else None\n",
    "\n",
    "# def extract_log_level(template):\n",
    "#     match = re.search(r\"\\b(INFO|WARN|ERROR|DEBUG|TRACE|CRITICAL)\\b\", template, re.IGNORECASE)\n",
    "#     return match.group(0).upper() if match else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560a935",
   "metadata": {},
   "source": [
    "### Main Enrichment Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9af85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def enrich_templates(df, dataset_name=\"generic\", known_services=None, preserve_symbols=\":=\"):\n",
    "    print(f\"üîß Enriching templates for dataset: {dataset_name}\")\n",
    "    \n",
    "    tqdm.pandas(desc=\"Cleaning templates\")\n",
    "    df[\"clean_template\"] = df[\"template\"].progress_apply(\n",
    "        lambda t: clean_template(t, preserve_symbols=preserve_symbols)\n",
    "    )\n",
    "    \n",
    "    tqdm.pandas(desc=\"Extracting service hints\")\n",
    "    if known_services is None:\n",
    "        known_services = df[\"template\"].str.extract(r\"([a-zA-Z0-9_-]+)\\.log\")[0].dropna().unique().tolist()\n",
    "    \n",
    "    df[\"service_hint\"] = df[\"clean_template\"].progress_apply(\n",
    "        lambda x: extract_service_hint(x, known_services)\n",
    "    )\n",
    "    \n",
    "    # tqdm.pandas(desc=\"Extracting timestamps\")\n",
    "    # df[\"timestamp\"] = df[\"clean_template\"].progress_apply(extract_timestamp)\n",
    "    \n",
    "    # tqdm.pandas(desc=\"Extracting log levels\")\n",
    "    # df[\"log_level\"] = df[\"clean_template\"].progress_apply(extract_log_level)\n",
    "    \n",
    "    print(\"‚úÖ Enrichment complete ‚Äî features added: clean_template, service_hint, timestamp, log_level\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b0f0e",
   "metadata": {},
   "source": [
    "## 2. Running the complete Enrichment Process and Saving the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca013085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell that Loads the Parsed log templates and runs the Enrichment function\n",
    "\n",
    "\n",
    "templates_df = pd.read_csv(f\"{config['DATA_DIR']}/{config['DATASET_NAME']}_full.log_templates.csv\")\n",
    "print(f\"Loaded {len(templates_df)} templates\")\n",
    "\n",
    "enriched_templates_df = enrich_templates(templates_df, dataset_name=config[\"DATASET_NAME\"], preserve_symbols=\":=\")\n",
    "\n",
    "enriched_templates_df.to_csv(f\"{config['DATA_DIR']}/{config['DATASET_NAME']}_full.log_enriched_templates.csv\", index=False)\n",
    "print(f\"‚úÖ Enriched templates saved to '{config['DATA_DIR']}/{config['DATASET_NAME']}_full.log_enriched_templates.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac06a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(templates_df[\"service_hint\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c14c0e",
   "metadata": {},
   "source": [
    "### Utility Function: Persist Known Services in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "\n",
    "# def update_known_services(new_services, services_path=f\"{config['DATA_DIR']}/config['KNOWN_SERVICES_FILE']\", verbose=True):\n",
    "\n",
    "    \n",
    "#     new_services = sorted(set(s.strip().lower() for s in new_services if s and isinstance(s, str)))\n",
    "\n",
    "#     # Load existing services if file exists\n",
    "#     if os.path.exists(services_path):\n",
    "#         try:\n",
    "#             with open(services_path, \"r\") as f:\n",
    "#                 existing_services = json.load(f)\n",
    "#         except Exception:\n",
    "#             existing_services = []\n",
    "#     else:\n",
    "#         existing_services = []\n",
    "\n",
    "#     # Merge and deduplicate\n",
    "#     updated_services = sorted(set(existing_services + new_services))\n",
    "\n",
    "#     # Determine if changes occurred\n",
    "#     new_added = [s for s in updated_services if s not in existing_services]\n",
    "\n",
    "#     if new_added:\n",
    "#         with open(services_path, \"w\") as f:\n",
    "#             json.dump(updated_services, f, indent=2)\n",
    "#         if verbose:\n",
    "#             print(f\"üÜï Added {len(new_added)} new services to {services_path}: {new_added}\")\n",
    "#     else:\n",
    "#         if verbose:\n",
    "#             print(f\"‚úÖ No new services found. Existing {len(existing_services)} services retained.\")\n",
    "\n",
    "#     return updated_services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the KNOWN SERVIES persistence function\n",
    "new_services = templates_df[\"service_hint\"].dropna().unique().tolist()\n",
    "\n",
    "# 2Ô∏è‚É£ Persist and auto-update known services registry\n",
    "# known_services = update_known_services(new_services)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870765e",
   "metadata": {},
   "source": [
    "#### Context Window Builder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd62ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "# from typing import Optional\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# def build_context_windows_tokenized(\n",
    "#     enriched_df: pd.DataFrame,\n",
    "#     service_col: str = \"service_hint\",\n",
    "#     template_col: str = \"template\",\n",
    "#     include_metadata: bool = True,\n",
    "#     max_templates_per_service: Optional[int] = None,\n",
    "#     tokenizer_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     max_tokens: int = 512,\n",
    "#     overlap_tokens: int = 50\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Build tokenized service-level context windows with optional metadata.\n",
    "#     Ensures no window exceeds max_tokens. Uses overlapping windows for context continuity.\n",
    "#     \"\"\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "#     service_contexts = defaultdict(list)\n",
    "\n",
    "#     print(\"üîß Building tokenized service-level context windows...\")\n",
    "#     for svc, svc_df in tqdm(enriched_df.groupby(service_col), desc=\"Processing services\"):\n",
    "#         context_lines = []\n",
    "\n",
    "#         # Step 1: Collect all snippets per service\n",
    "#         for _, row in svc_df.iterrows():\n",
    "#             snippet = row[template_col]\n",
    "#             if include_metadata:\n",
    "#                 meta_fields = {k: v for k, v in row.items() if k not in [service_col, template_col]}\n",
    "#                 meta_str = \", \".join(f\"{k}: {v}\" for k, v in meta_fields.items() if pd.notna(v))\n",
    "#                 snippet = f\"[Template: {snippet}] [{meta_str}]\" if meta_str else snippet\n",
    "#             context_lines.append(snippet)\n",
    "\n",
    "#         # Step 2: Trim large services if needed\n",
    "#         if max_templates_per_service:\n",
    "#             context_lines = context_lines[:max_templates_per_service]\n",
    "\n",
    "#         # Step 3: Build token windows\n",
    "#         win_id = 1\n",
    "#         current_tokens = []\n",
    "#         for snippet in context_lines:\n",
    "#             snippet_tokens = tokenizer.encode(snippet, add_special_tokens=False)\n",
    "#             while len(snippet_tokens) > 0:\n",
    "#                 space_left = max_tokens - len(current_tokens)\n",
    "#                 take_tokens = snippet_tokens[:space_left]\n",
    "#                 current_tokens.extend(take_tokens)\n",
    "#                 snippet_tokens = snippet_tokens[space_left:]\n",
    "\n",
    "#                 if len(current_tokens) >= max_tokens:\n",
    "#                     # Create window\n",
    "#                     window_text = tokenizer.decode(current_tokens, skip_special_tokens=True)\n",
    "#                     service_contexts[svc].append({\"window_id\": win_id, \"context_text\": window_text})\n",
    "#                     win_id += 1\n",
    "\n",
    "#                     # Prepare next window with overlap\n",
    "#                     if overlap_tokens > 0:\n",
    "#                         current_tokens = current_tokens[-overlap_tokens:]\n",
    "#                     else:\n",
    "#                         current_tokens = []\n",
    "\n",
    "#         # Step 4: Save any remaining tokens as the last window\n",
    "#         if current_tokens:\n",
    "#             window_text = tokenizer.decode(current_tokens, skip_special_tokens=True)\n",
    "#             service_contexts[svc].append({\"window_id\": win_id, \"context_text\": window_text})\n",
    "\n",
    "#     total_windows = sum(len(wins) for wins in service_contexts.values())\n",
    "#     print(\"‚úÖ Tokenized context windows complete.\")\n",
    "#     print(f\"üì¶ Generated {total_windows} total context windows.\\n\")\n",
    "\n",
    "#     return dict(service_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e842d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Building tokenized service-level context windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing services: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 151.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized context windows complete.\n",
      "üì¶ Generated 25 total context windows.\n",
      "\n",
      "[INFO] Constructed 5 service-level context groups.\n",
      "\n",
      "üîπ Service: api | Total windows: 1\n",
      "Window 1 (122 tokens preview):\n",
      "'[ template : nova. api. openstack. compute. server _ external _ events < * > * > f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - - ] creating event < * > * > for instance < * > *'\n",
      "‚Ä¶\n",
      "\n",
      "üîπ Service: compute | Total windows: 18\n",
      "Window 1 (266 tokens preview):\n",
      "'[ template : nova. compute. manager [ req - 3ea4052c - 895d - 4b64 - 9e2d - 04d64c4d94ab - - - - - ] [ instance : < * > * > vm < * > * > ( lifecycle event ) ] [ count : 88, clean _ template : nova. co'\n",
      "‚Ä¶\n",
      "Window 2 (184 tokens preview):\n",
      "'##1fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - - ] [ instance : < * > * > terminating instance ] [ count : 22, clean _ template : nova. compute. manager 113d3a99c3da401fbd62cc2caa5b96d2 54f'\n",
      "‚Ä¶\n",
      "\n",
      "üîπ Service: nova | Total windows: 4\n",
      "Window 1 (303 tokens preview):\n",
      "'[ template : nova. virt. libvirt. imagecache [ req - addc1839 - 2ed5 - 4778 - b57e - 5854eb7b8b09 - - - - - ] < * > * > base < * > * > / var / lib / nova / instances / _ base / a489c868f0c37da93b76227'\n",
      "‚Ä¶\n",
      "Window 2 (238 tokens preview):\n",
      "'##b - 86c4 - 40623fbe45b4 at ( / var / lib / nova / instances / _ base / a489c868f0c37da93b76227c91bb03908ac0e742 ) : in use : on this node 1 local, 0 on other nodes sharing this instance storage ] [ '\n",
      "‚Ä¶\n",
      "\n",
      "üîπ Service: osapi_compute | Total windows: 1\n",
      "Window 1 (194 tokens preview):\n",
      "'[ template : nova. osapi _ compute. wsgi. server < * > * > < * > * > < * > * > - - - ] < * > * > \" get < * > * > http / 1. 1 \" status : 200 len : < * > * > time : < * > * > ] [ count : 723, clean _ te'\n",
      "‚Ä¶\n",
      "\n",
      "üîπ Service: scheduler | Total windows: 1\n",
      "Window 1 (158 tokens preview):\n",
      "\"[ template : nova. scheduler. host _ manager < * > * > - - - - - ] the instance sync for host ' cp - 1. slowvm1. tcloud - pg0. utah. cloudlab. us ' did not match. re - created its instancelist. ] [ co\"\n",
      "‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening tokenized service contexts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 39053.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Tokenized context windows saved to:\n",
      " - CSV: data/contexts/context_windows_20251023_194121.csv\n",
      " - JSONL: data/contexts/context_windows_20251023_194121.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Build token-aware context windows\n",
    "# from tqdm import tqdm \n",
    "\n",
    "# service_contexts = build_context_windows_tokenized(\n",
    "#     enriched_df,\n",
    "#     service_col=\"service_hint\",\n",
    "#     template_col=\"template\",\n",
    "#     include_metadata=True,\n",
    "#     max_templates_per_service=None,  # adjust if needed\n",
    "#     max_tokens=512,\n",
    "#     overlap_tokens=50\n",
    "# )\n",
    "\n",
    "# print(f\"[INFO] Constructed {len(service_contexts)} service-level context groups.\")\n",
    "\n",
    "# # Preview first 1‚Äì2 windows per service for sanity check\n",
    "# for svc, windows in list(service_contexts.items())[:5]:  # first 5 services\n",
    "#     print(f\"\\nüîπ Service: {svc} | Total windows: {len(windows)}\")\n",
    "#     for win in windows[:2]:  # show first 2 windows\n",
    "#         print(f\"Window {win['window_id']} ({len(win['context_text'].split())} tokens preview):\")\n",
    "#         print(repr(win['context_text'][:200]))  # first 200 chars\n",
    "#         print(\"‚Ä¶\")\n",
    "\n",
    "\n",
    "# # Flatten token windows for saving\n",
    "# records = []\n",
    "\n",
    "# for svc, windows in tqdm(service_contexts.items(), desc=\"Flattening tokenized service contexts\"):\n",
    "#     for win in windows:\n",
    "#         records.append({\n",
    "#             \"service\": svc,\n",
    "#             \"window_id\": win[\"window_id\"],\n",
    "#             \"context_text\": win[\"context_text\"]\n",
    "#         })\n",
    "\n",
    "# context_windows_df = pd.DataFrame(records)\n",
    "\n",
    "# # SAVING CONTEXT WINDOWS TO CSV/JSONL\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "# os.makedirs(f\"{config['DATA_DIR']}/{config['CONTEXT_DIR']}\", exist_ok=True)\n",
    "\n",
    "# # Timestamped filenames for traceability\n",
    "# timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# context_path_csv = os.path.join(f\"{config['DATA_DIR']}/{config['CONTEXT_DIR']}\", f\"context_windows_{timestamp_str}.csv\")\n",
    "# context_path_jsonl = os.path.join(f\"{config['DATA_DIR']}/{config['CONTEXT_DIR']}\", f\"context_windows_{timestamp_str}.jsonl\")\n",
    "\n",
    "# context_windows_df.to_csv(context_path_csv, index=False)\n",
    "# context_windows_df.to_json(context_path_jsonl, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "# print(f\"üíæ Tokenized context windows saved to:\\n - CSV: {context_path_csv}\\n - JSONL: {context_path_jsonl}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
