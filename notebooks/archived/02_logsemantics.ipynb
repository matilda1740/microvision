{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacbd45b",
   "metadata": {},
   "source": [
    "### MicroVision \n",
    "\n",
    "#### Generating Semantic Log Template Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a264fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed!!\n",
      "Current working directory: /Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd() == '/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision/notebooks':\n",
    "    os.chdir('/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision')\n",
    "    print(\"Changed!!\")\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f081d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy tqdm matplotlib sentence-transformers chromadb --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991c758",
   "metadata": {},
   "source": [
    "#### Loading the enriched Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b260528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enriched data loaded successfully: 207632 rows √ó 15 columns\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "line_no",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "raw",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "template_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "template",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "File",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Component",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "semantic_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "structured_metadata",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "fb4d90d2-c116-454f-a1ac-8e46a120da0a",
       "rows": [
        [
         "0",
         "1",
         "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.445 25746 INFO nova.osapi_compute.wsgi.server [req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1919448",
         "[req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1919448",
         "1",
         "[<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*><IP>> <*>*> <*>*> HTTP/1.1\" status: <*>*> len: <*>*> time: <*>*>",
         "nova-api.log.2017-05-14_21:27:04",
         "2017-05-14",
         "19:39:01.445",
         "25746",
         "INFO",
         "nova.osapi_compute.wsgi.server",
         "[req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1919448",
         "[nova.osapi_compute.wsgi.server] [INFO] [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*><IP>> <*>*> <*>*> HTTP/1.1\" status: <*>*> len: <*>*> time: <*>*>",
         "{'component': 'nova.osapi_compute.wsgi.server', 'level': 'INFO', 'pid': '25746', 'timestamp': Timestamp('2017-05-14 19:39:01.445000')}",
         "2017-05-14 19:39:01.445"
        ],
        [
         "1",
         "2",
         "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:01.650 25746 INFO nova.osapi_compute.wsgi.server [req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/3edec1e4-9678-4a3a-a21b-a145a4ee5e61 HTTP/1.1\" status: 200 len: 1708 time: 0.2011580",
         "[req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/3edec1e4-9678-4a3a-a21b-a145a4ee5e61 HTTP/1.1\" status: 200 len: 1708 time: 0.2011580",
         "1",
         "[<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*><IP>> <*>*> <*>*> HTTP/1.1\" status: <*>*> len: <*>*> time: <*>*>",
         "nova-api.log.2017-05-14_21:27:04",
         "2017-05-14",
         "19:39:01.650",
         "25746",
         "INFO",
         "nova.osapi_compute.wsgi.server",
         "[req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/3edec1e4-9678-4a3a-a21b-a145a4ee5e61 HTTP/1.1\" status: 200 len: 1708 time: 0.2011580",
         "[nova.osapi_compute.wsgi.server] [INFO] [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*><IP>> <*>*> <*>*> HTTP/1.1\" status: <*>*> len: <*>*> time: <*>*>",
         "{'component': 'nova.osapi_compute.wsgi.server', 'level': 'INFO', 'pid': '25746', 'timestamp': Timestamp('2017-05-14 19:39:01.650000')}",
         "2017-05-14 19:39:01.650"
        ],
        [
         "2",
         "3",
         "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:02.007 2931 INFO nova.virt.libvirt.driver [req-e285b551-587f-4c1d-8eba-dceb2673637f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 3edec1e4-9678-4a3a-a21b-a145a4ee5e61] Creating image",
         "[req-e285b551-587f-4c1d-8eba-dceb2673637f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 3edec1e4-9678-4a3a-a21b-a145a4ee5e61] Creating image",
         "2",
         "[<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] [instance: <*>*> <*>*> <*>*>",
         "nova-compute.log.2017-05-14_21:27:09",
         "2017-05-14",
         "19:39:02.007",
         "2931",
         "INFO",
         "nova.virt.libvirt.driver",
         "[req-e285b551-587f-4c1d-8eba-dceb2673637f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 3edec1e4-9678-4a3a-a21b-a145a4ee5e61] Creating image",
         "[nova.virt.libvirt.driver] [INFO] [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] [instance: <*>*> <*>*> <*>*>",
         "{'component': 'nova.virt.libvirt.driver', 'level': 'INFO', 'pid': '2931', 'timestamp': Timestamp('2017-05-14 19:39:02.007000')}",
         "2017-05-14 19:39:02.007"
        ],
        [
         "3",
         "4",
         "nova-api.log.2017-05-14_21:27:04 2017-05-14 19:39:02.924 25746 INFO nova.osapi_compute.wsgi.server [req-eb681812-78ae-4a9f-9e2a-96e505285512 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.2698390",
         "[req-eb681812-78ae-4a9f-9e2a-96e505285512 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.2698390",
         "1",
         "[<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*><IP>> <*>*> <*>*> HTTP/1.1\" status: <*>*> len: <*>*> time: <*>*>",
         "nova-api.log.2017-05-14_21:27:04",
         "2017-05-14",
         "19:39:02.924",
         "25746",
         "INFO",
         "nova.osapi_compute.wsgi.server",
         "[req-eb681812-78ae-4a9f-9e2a-96e505285512 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.2698390",
         "[nova.osapi_compute.wsgi.server] [INFO] [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*><IP>> <*>*> <*>*> HTTP/1.1\" status: <*>*> len: <*>*> time: <*>*>",
         "{'component': 'nova.osapi_compute.wsgi.server', 'level': 'INFO', 'pid': '25746', 'timestamp': Timestamp('2017-05-14 19:39:02.924000')}",
         "2017-05-14 19:39:02.924"
        ],
        [
         "4",
         "5",
         "nova-compute.log.2017-05-14_21:27:09 2017-05-14 19:39:03.166 2931 INFO nova.compute.manager [-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2f4b25] VM Stopped (Lifecycle Event)",
         "[-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2f4b25] VM Stopped (Lifecycle Event)",
         "3",
         "[-] [instance: <*>*> VM Stopped (Lifecycle Event)",
         "nova-compute.log.2017-05-14_21:27:09",
         "2017-05-14",
         "19:39:03.166",
         "2931",
         "INFO",
         "nova.compute.manager",
         "[-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2f4b25] VM Stopped (Lifecycle Event)",
         "[nova.compute.manager] [INFO] [-] [instance: <*>*> VM Stopped (Lifecycle Event)",
         "{'component': 'nova.compute.manager', 'level': 'INFO', 'pid': '2931', 'timestamp': Timestamp('2017-05-14 19:39:03.166000')}",
         "2017-05-14 19:39:03.166"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>raw</th>\n",
       "      <th>content</th>\n",
       "      <th>template_id</th>\n",
       "      <th>template</th>\n",
       "      <th>File</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "      <th>semantic_text</th>\n",
       "      <th>structured_metadata</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nova-api.log.2017-05-14_21:27:04 2017-05-14 19...</td>\n",
       "      <td>[req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;*&gt;&lt;REQ_ID&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; - - -] &lt;*&gt;...</td>\n",
       "      <td>nova-api.log.2017-05-14_21:27:04</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>19:39:01.445</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>[req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d...</td>\n",
       "      <td>[nova.osapi_compute.wsgi.server] [INFO] [&lt;*&gt;&lt;R...</td>\n",
       "      <td>{'component': 'nova.osapi_compute.wsgi.server'...</td>\n",
       "      <td>2017-05-14 19:39:01.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>nova-api.log.2017-05-14_21:27:04 2017-05-14 19...</td>\n",
       "      <td>[req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;*&gt;&lt;REQ_ID&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; - - -] &lt;*&gt;...</td>\n",
       "      <td>nova-api.log.2017-05-14_21:27:04</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>19:39:01.650</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>[req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d...</td>\n",
       "      <td>[nova.osapi_compute.wsgi.server] [INFO] [&lt;*&gt;&lt;R...</td>\n",
       "      <td>{'component': 'nova.osapi_compute.wsgi.server'...</td>\n",
       "      <td>2017-05-14 19:39:01.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>nova-compute.log.2017-05-14_21:27:09 2017-05-1...</td>\n",
       "      <td>[req-e285b551-587f-4c1d-8eba-dceb2673637f 113d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[&lt;*&gt;&lt;REQ_ID&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; - - -] [in...</td>\n",
       "      <td>nova-compute.log.2017-05-14_21:27:09</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>19:39:02.007</td>\n",
       "      <td>2931</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.virt.libvirt.driver</td>\n",
       "      <td>[req-e285b551-587f-4c1d-8eba-dceb2673637f 113d...</td>\n",
       "      <td>[nova.virt.libvirt.driver] [INFO] [&lt;*&gt;&lt;REQ_ID&gt;...</td>\n",
       "      <td>{'component': 'nova.virt.libvirt.driver', 'lev...</td>\n",
       "      <td>2017-05-14 19:39:02.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>nova-api.log.2017-05-14_21:27:04 2017-05-14 19...</td>\n",
       "      <td>[req-eb681812-78ae-4a9f-9e2a-96e505285512 113d...</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;*&gt;&lt;REQ_ID&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; &lt;*&gt;&lt;HASH&gt;&gt; - - -] &lt;*&gt;...</td>\n",
       "      <td>nova-api.log.2017-05-14_21:27:04</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>19:39:02.924</td>\n",
       "      <td>25746</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.osapi_compute.wsgi.server</td>\n",
       "      <td>[req-eb681812-78ae-4a9f-9e2a-96e505285512 113d...</td>\n",
       "      <td>[nova.osapi_compute.wsgi.server] [INFO] [&lt;*&gt;&lt;R...</td>\n",
       "      <td>{'component': 'nova.osapi_compute.wsgi.server'...</td>\n",
       "      <td>2017-05-14 19:39:02.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>nova-compute.log.2017-05-14_21:27:09 2017-05-1...</td>\n",
       "      <td>[-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-] [instance: &lt;*&gt;*&gt; VM Stopped (Lifecycle Event)</td>\n",
       "      <td>nova-compute.log.2017-05-14_21:27:09</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>19:39:03.166</td>\n",
       "      <td>2931</td>\n",
       "      <td>INFO</td>\n",
       "      <td>nova.compute.manager</td>\n",
       "      <td>[-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2...</td>\n",
       "      <td>[nova.compute.manager] [INFO] [-] [instance: &lt;...</td>\n",
       "      <td>{'component': 'nova.compute.manager', 'level':...</td>\n",
       "      <td>2017-05-14 19:39:03.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   line_no                                                raw  \\\n",
       "0        1  nova-api.log.2017-05-14_21:27:04 2017-05-14 19...   \n",
       "1        2  nova-api.log.2017-05-14_21:27:04 2017-05-14 19...   \n",
       "2        3  nova-compute.log.2017-05-14_21:27:09 2017-05-1...   \n",
       "3        4  nova-api.log.2017-05-14_21:27:04 2017-05-14 19...   \n",
       "4        5  nova-compute.log.2017-05-14_21:27:09 2017-05-1...   \n",
       "\n",
       "                                             content  template_id  \\\n",
       "0  [req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d...            1   \n",
       "1  [req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d...            1   \n",
       "2  [req-e285b551-587f-4c1d-8eba-dceb2673637f 113d...            2   \n",
       "3  [req-eb681812-78ae-4a9f-9e2a-96e505285512 113d...            1   \n",
       "4  [-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2...            3   \n",
       "\n",
       "                                            template  \\\n",
       "0  [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*>...   \n",
       "1  [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*>...   \n",
       "2  [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] [in...   \n",
       "3  [<*><REQ_ID>> <*><HASH>> <*><HASH>> - - -] <*>...   \n",
       "4  [-] [instance: <*>*> VM Stopped (Lifecycle Event)   \n",
       "\n",
       "                                   File        Date          Time    Pid  \\\n",
       "0      nova-api.log.2017-05-14_21:27:04  2017-05-14  19:39:01.445  25746   \n",
       "1      nova-api.log.2017-05-14_21:27:04  2017-05-14  19:39:01.650  25746   \n",
       "2  nova-compute.log.2017-05-14_21:27:09  2017-05-14  19:39:02.007   2931   \n",
       "3      nova-api.log.2017-05-14_21:27:04  2017-05-14  19:39:02.924  25746   \n",
       "4  nova-compute.log.2017-05-14_21:27:09  2017-05-14  19:39:03.166   2931   \n",
       "\n",
       "  Level                       Component  \\\n",
       "0  INFO  nova.osapi_compute.wsgi.server   \n",
       "1  INFO  nova.osapi_compute.wsgi.server   \n",
       "2  INFO        nova.virt.libvirt.driver   \n",
       "3  INFO  nova.osapi_compute.wsgi.server   \n",
       "4  INFO            nova.compute.manager   \n",
       "\n",
       "                                             Content  \\\n",
       "0  [req-5a2050e7-b381-4ae9-92d2-8b08e9f9f4c0 113d...   \n",
       "1  [req-c26a7d54-55ab-412e-947f-421a2cb934fc 113d...   \n",
       "2  [req-e285b551-587f-4c1d-8eba-dceb2673637f 113d...   \n",
       "3  [req-eb681812-78ae-4a9f-9e2a-96e505285512 113d...   \n",
       "4  [-] [instance: 2b590f10-49fd-4ec9-ae41-19596c2...   \n",
       "\n",
       "                                       semantic_text  \\\n",
       "0  [nova.osapi_compute.wsgi.server] [INFO] [<*><R...   \n",
       "1  [nova.osapi_compute.wsgi.server] [INFO] [<*><R...   \n",
       "2  [nova.virt.libvirt.driver] [INFO] [<*><REQ_ID>...   \n",
       "3  [nova.osapi_compute.wsgi.server] [INFO] [<*><R...   \n",
       "4  [nova.compute.manager] [INFO] [-] [instance: <...   \n",
       "\n",
       "                                 structured_metadata                Timestamp  \n",
       "0  {'component': 'nova.osapi_compute.wsgi.server'...  2017-05-14 19:39:01.445  \n",
       "1  {'component': 'nova.osapi_compute.wsgi.server'...  2017-05-14 19:39:01.650  \n",
       "2  {'component': 'nova.virt.libvirt.driver', 'lev...  2017-05-14 19:39:02.007  \n",
       "3  {'component': 'nova.osapi_compute.wsgi.server'...  2017-05-14 19:39:02.924  \n",
       "4  {'component': 'nova.compute.manager', 'level':...  2017-05-14 19:39:03.166  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "config = {\n",
    "        \"DATA_DIR\": \"data\",\n",
    "        \"DATASET_NAME\": \"OpenStack\",\n",
    "        \"DATASET_LOG\": \"_full.log\",\n",
    "        \"ENRICHED_CSV\": \"_enriched.csv\",\n",
    "        \"TEMPLATES_CSV\": \"_templates.csv\",\n",
    "        \"TEMPLATES_JSON\": \"_templates.json\",\n",
    "        \"PERSISTENCE_PATH\": \"persistence\",\n",
    "        \"DRAIN_PATH\": \"drain3_state\",\n",
    "                \n",
    "        \"DEDUP_CSV\": \"_deduplicated.csv\",\n",
    "        \"EMBEDDINGS_CSV\": \"_embeddings.csv\",\n",
    "        \"CHROMA_PERSIST_DIR\": \"chroma_storage\",\n",
    "        \"COLLECTION_NAME\": \"_collection\"\n",
    "}\n",
    "\n",
    "os.makedirs(config[\"DATA_DIR\"], exist_ok=True)\n",
    "\n",
    "enriched_df = pd.read_csv(f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['ENRICHED_CSV']}\")\n",
    "print(f\"‚úÖ Enriched data loaded successfully: {enriched_df.shape[0]} rows √ó {enriched_df.shape[1]} columns\")\n",
    "enriched_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d5978",
   "metadata": {},
   "source": [
    "### Step 2: Generate Semantic Embeddings\n",
    "\n",
    "#### Sentence Transformer used: *** sentence-transformers/multi-qa-mpnet-base-dot-v1 ***\n",
    "\n",
    "Initial Model used - ** all-MiniLM-L6-v2 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e10b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matildamwendwa/Desktop/Desktop - Admin‚Äôs MacBook Pro/Python_Projects/microvision/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded Sentence Transformer model: all-mpnet-base-v2, using device: mps\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# model_name = \"all-MiniLM-L6-v2\"\n",
    "# model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "print(f\"‚úÖ Loaded Sentence Transformer model: {model_name}, using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485d9ab",
   "metadata": {},
   "source": [
    "### ENCODING THE TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d3ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Deduplication based on SHA1 hash: 151,354 unique log lines\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Deduplication based on SHA1 hash\n",
    "import hashlib\n",
    "# Compute SHA1 hash for fast duplicate detection\n",
    "enriched_df[\"hash\"] = enriched_df[\"Content\"].apply(lambda x: hashlib.sha1(x.encode('utf-8')).hexdigest())\n",
    "hashed_df = enriched_df.drop_duplicates(subset=\"hash\").drop(columns=[\"hash\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"After Deduplication based on SHA1 hash: {len(hashed_df):,} unique log lines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a19712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Generating embeddings for enriched templates...\n",
      "üöÄ Starting encoding for 151,354 log lines in 592 batches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Logs in Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 592/592 [1:45:05<00:00, 10.65s/batch]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Final embeddings saved to data/embeddings_stream\n",
      "‚úÖ Generated embeddings for 151,354 log lines\n",
      "‚úÖ Stored 151354 enriched templates with embeddings ‚Üí data/OpenStack_full.log_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_encode_logs(df, text_column, batch_size, checkpoint_path):\n",
    "\n",
    "    print(\"üîß Generating embeddings for enriched templates...\")\n",
    "\n",
    "    # --- Generate embeddings ---\n",
    "    logs = df[text_column].tolist()\n",
    "    total_batches = int(np.ceil(len(logs) / batch_size))\n",
    "    print(f\"üöÄ Starting encoding for {len(logs):,} log lines in {total_batches} batches...\\n\")\n",
    "\n",
    "    # Load previous checkpoint if available\n",
    "    start_idx = 0\n",
    "    embeddings = []\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"üîÅ Found checkpoint: {checkpoint_path}. Resuming...\")\n",
    "        checkpoint = np.load(checkpoint_path, allow_pickle=True)\n",
    "        embeddings = [torch.tensor(checkpoint[\"embeddings\"])]\n",
    "        start_idx = int(checkpoint[\"index\"])\n",
    "        print(f\"Resuming from batch {start_idx:,}\")\n",
    "\n",
    "    # --- Batch encoding loop ---\n",
    "    for batch_idx in tqdm(range(start_idx, len(logs), batch_size), desc=\"Encoding Logs in Batches\", unit=\"batch\"):\n",
    "\n",
    "        batch_logs = logs[batch_idx: batch_idx + batch_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode(batch_logs, device=device,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True,\n",
    "\n",
    "            )        \n",
    "        embeddings.append(batch_embeddings.cpu())\n",
    "\n",
    "         # Save checkpoint every N batches\n",
    "        if (batch_idx // batch_size + 1) % 50 == 0 or (batch_idx + batch_size) >= total_batches:\n",
    "            stacked = torch.cat(embeddings, dim=0).cpu().numpy()\n",
    "            np.savez_compressed(checkpoint_path, embeddings=stacked, index=batch_idx + batch_size)\n",
    "            # ---- CLEAN UP GPU\n",
    "            embeddings = [torch.tensor(stacked)]  # keep single tensor to reduce many small tensors\n",
    "            torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    # STACK ALL BATCHES\n",
    "    embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "    np.savez_compressed(checkpoint_path.replace(\"_stream.npz\", \"_final.npz\"), embeddings=embeddings)\n",
    "    print(f\"üì¶ Final embeddings saved to {checkpoint_path.replace('_stream.npz', '_final.npz')}\")\n",
    "\n",
    "    df[\"embedding\"] = [emb.tolist() for emb in embeddings]\n",
    "\n",
    "    df[\"embedding_id\"] = [\n",
    "        f\"{r['Component']}_{r['template_id']}_{i}\" if \"template_id\" in r else f\"{r['Component']}_{i}\"\n",
    "        for i, r in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    print(f\"‚úÖ Generated embeddings for {len(df):,} log lines\")\n",
    "    return df, embeddings\n",
    "\n",
    "\n",
    "df, embeddings = batch_encode_logs(\n",
    "    hashed_df, \n",
    "    text_column=\"semantic_text\",\n",
    "    batch_size=256, \n",
    "    checkpoint_path=f\"{config['DATA_DIR']}/embeddings_stream\",\n",
    ")\n",
    "\n",
    "# # --- Persist ---\n",
    "EMBEDDINGS_CSV_PATH = f\"{config['DATA_DIR']}/{config['DATASET_NAME']}{config['DATASET_LOG']}{config['EMBEDDINGS_CSV']}\"\n",
    "df.to_csv(EMBEDDINGS_CSV_PATH, index=False)\n",
    "print(f\"‚úÖ Stored {len(df)} enriched templates with embeddings ‚Üí {EMBEDDINGS_CSV_PATH}\")\n",
    "\n",
    "# # --- Preview ---\n",
    "# print(enriched_df[[\"Component\", \"semantic_text\", \"structured_metadata\", \"Timestamp\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec770cc1",
   "metadata": {},
   "source": [
    "### Step 3: Store Embeddings in a Vector Database (ChromaDB)\n",
    "\n",
    "#### 3.1 Initializing ChromaDB and some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# -------- Initializing ChromaDB Collection --------\n",
    "\n",
    "def init_chroma_collection(\n",
    "    persist_dir: str,\n",
    "    collection_name: str,\n",
    "    space: str = \"cosine\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize persistent client\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    # Retrieve or create collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": space}\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üîÅ Loaded persistent Chroma from: {persist_dir}\")\n",
    "        print(f\"üß© Available collections: '{collection.name}' ‚Üí Contains {collection.count()} embeddings\")\n",
    "        print(\"üß© Existing collections:\", [c.name for c in client.list_collections()])\n",
    "\n",
    "\n",
    "    return client, collection\n",
    "\n",
    "# -------- SAFE PARSING FUNCTION \n",
    "       \n",
    "def safe_parse_embedding(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(x)\n",
    "            if isinstance(val, list) and len(val) == 1 and isinstance(val[0], str):\n",
    "                return ast.literal_eval(val[0])\n",
    "            return val\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def safe_parse_metadata(x):\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        # Try JSON first (more robust)\n",
    "        try:\n",
    "            return json.loads(x)\n",
    "        except Exception:\n",
    "            # Try literal_eval fallback\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except Exception:\n",
    "                return {\"raw_metadata\": x}  # fallback\n",
    "    return {\"raw_metadata\": str(x)}\n",
    "\n",
    "# -------- Repopulating ChromaDB from DataFrame --------\n",
    "\n",
    "def repopulate_chroma_from_df(client, collection, df, batch_size):\n",
    "\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(safe_parse_embedding)\n",
    "\n",
    "    ids = df[\"embedding_id\"].tolist()\n",
    "    texts = df[\"semantic_text\"].tolist()\n",
    "    embeddings = df[\"embedding\"].tolist()\n",
    "\n",
    "    # Build metadata dictionary - prefer normalized metadata with ISO timestamps\n",
    "    from src.enrichment.ingest_utils import df_to_metadatas\n",
    "\n",
    "    if \"structured_metadata\" in df.columns:\n",
    "        # parse existing structured metadata\n",
    "        metadatas = df[\"structured_metadata\"].apply(safe_parse_metadata).tolist()\n",
    "        # if structured metadata lacks timestamp normalization but df has a timestamp column,\n",
    "        # fall back to serializing rows to ensure consistent ISO-8601 timestamps\n",
    "        if any(\n",
    "            (not isinstance(m, dict) or all(k not in m for k in (\"timestamp\", \"time\", \"ts\", \"created_at\")))\n",
    "            for m in metadatas\n",
    "        ) and any(c in df.columns for c in (\"timestamp\", \"time\", \"ts\", \"created_at\")):\n",
    "            meta_cols = [col for col in df.columns if col not in [\"embedding\", \"semantic_text\", \"structured_metadata\", \"Timestamp\"]]\n",
    "            metadatas = df_to_metadatas(df, meta_cols=meta_cols)\n",
    "    else:\n",
    "        meta_cols = [col for col in df.columns if col not in [\"embedding\", \"semantic_text\", \"structured_metadata\", \"Timestamp\"]]\n",
    "        metadatas = df_to_metadatas(df, meta_cols=meta_cols)\n",
    "\n",
    "    print(f\"‚úÖ Inserting {len(texts)} semantic embeddings into ChromaDB (batch size={batch_size})\")\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"üöÄ Populating Chroma\"):\n",
    "        collection.add(\n",
    "            documents=texts[i:i+batch_size],\n",
    "            ids=ids[i:i+batch_size],\n",
    "            embeddings=embeddings[i:i+batch_size],\n",
    "            metadatas=metadatas[i:i+batch_size]\n",
    "        )\n",
    "\n",
    "    print(f\"‚úÖ Added {len(texts)} embeddings.\")\n",
    "    print(f\"üì¶ Collection now contains {collection.count()} vectors.\")\n",
    "    # print(\"üíæ Persistence handled automatically by Chroma ‚â•0.5.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d55d4",
   "metadata": {},
   "source": [
    "### 3.2 Run the Persistent Chroma Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c5d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loaded persistent Chroma from: data/persistence/chroma_storage\n",
      "üß© Available collections: 'OpenStack_full.log_collection' ‚Üí Contains 0 embeddings\n",
      "üß© Existing collections: ['OpenStack_full.log_collection']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re, ast, pandas as pd\n",
    "\n",
    "chroma_persist_dir = f\"{config['DATA_DIR']}/{config['PERSISTENCE_PATH']}/{config['CHROMA_PERSIST_DIR']}\"\n",
    "collection_name = f\"{config['DATASET_NAME']}{config['DATASET_LOG']}{config['COLLECTION_NAME']}\"\n",
    "\n",
    "client, collection = init_chroma_collection(chroma_persist_dir, collection_name)\n",
    "\n",
    "\n",
    "embeddings_df = pd.read_csv(EMBEDDINGS_CSV_PATH)\n",
    "# data = np.load(f\"{config['DATA_DIR']}/embeddings_stream.npz\")\n",
    "# embeddings = data[\"embeddings\"]\n",
    "# assert len(df) == len(embeddings), \"Mismatch between DF and embeddings length\"\n",
    "# df[\"embedding\"] = [emb.tolist() for emb in embeddings]\n",
    "\n",
    "repopulate_chroma_from_df(client, collection, embeddings_df, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bc2f1",
   "metadata": {},
   "source": [
    "##### Testing whether the Retrieval and Inference Stage works using sample tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_components = embeddings_df[\"Component\"].unique()[:3]\n",
    "print(f\"üß© Testing semantic retrieval for components: {list(sample_components)}\")\n",
    "\n",
    "for comp in tqdm(sample_components, desc=\"üîç Validating retrieval per service\"):\n",
    "    query_row = embeddings_df[embeddings_df[\"Component\"] == comp].sample(1).iloc[0]\n",
    "    query_text = query_row[\"semantic_text\"]\n",
    "    query_embedding = query_row[\"embedding\"]\n",
    "    query_id = query_row.get(\"embedding_id\", f\"{query_row['Component']}_{query_row.get('template_id','0')}\")\n",
    "\n",
    "    print(f\"\\nüß† Service: {comp}\")\n",
    "    print(f\"üìú Query Template: {query_text}\")\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=5,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nüîé Top 5 Semantic Matches:\")\n",
    "    for rank, (doc, meta, dist) in enumerate(\n",
    "        zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]), start=1\n",
    "    ):\n",
    "        score = 1 - dist  # convert to similarity\n",
    "        summary = f\"  {rank}. [{meta.get('component', '?')}] ‚Üí {doc[:80]}... (distance={dist:.4f}, sim={score:.4f})\"\n",
    "        if \"status\" in meta or \"method\" in meta:\n",
    "            summary += f\" [{meta.get('method','')}] [{meta.get('status','')}]\"\n",
    "        print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
